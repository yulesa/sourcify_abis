{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sourcify Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the Sourcify manifest.json file. Sourcify link: https://docs.sourcify.dev/docs/repository/sourcify-database/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -L -O https://repo-backup.sourcify.dev/manifest.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the manifest.json file to download only files from compiled_contracts, contract_deployments, verified_contracts.\n",
    "\n",
    "Download the files from the manifest.json using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jq -r '.files[].path' manifest.json | xargs -I {} curl -L -O https://repo-backup.sourcify.dev/{}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move each file to the corresponding folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Script manipulating the files to extract the abi in the desired format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import glaciers as gl\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unecessary columns from each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('compiled_contracts'):\n",
    "    if file.endswith('.parquet'):\n",
    "        df = pl.read_parquet(f'compiled_contracts/{file}')\n",
    "        df = df.select(\n",
    "            pl.col('id')\n",
    "            , pl.col('name')\n",
    "            , pl.col('fully_qualified_name')\n",
    "            , pl.col('compilation_artifacts').str.json_path_match('$.abi').alias('abi')\n",
    "        ).with_columns(\n",
    "            pl.col('abi').hash().alias('abi_id')\n",
    "        )\n",
    "        df.write_parquet(f'compiled_contracts/{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('contract_deployments'):\n",
    "    if file.endswith('.parquet'):\n",
    "        df = pl.read_parquet(f'contract_deployments/{file}')\n",
    "        df = df.select(\n",
    "            pl.col('id')\n",
    "            , pl.col('chain_id')\n",
    "            , pl.col('address')\n",
    "        )\n",
    "        df.write_parquet(f'contract_deployments/{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('verified_contracts'):\n",
    "    if file.endswith('.parquet'):\n",
    "        df = pl.read_parquet(f'verified_contracts/{file}')\n",
    "        df = df.select(\n",
    "            pl.col('id')\n",
    "            , pl.col('deployment_id')\n",
    "            , pl.col('compilation_id')\n",
    "        )\n",
    "        df.write_parquet(f'verified_contracts/{file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframes from the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "abis_df = pl.scan_parquet('compiled_contracts/compiled_contracts_*.parquet').rename({'id': 'compilation_id'}).collect()\n",
    "contract_ids_df = pl.scan_parquet('verified_contracts/verified_contracts_*.parquet').collect()\n",
    "deployment_ids_df = pl.scan_parquet('contract_deployments/contract_deployments_*.parquet').rename({'id': 'deployment_id'}).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "abis_df = abis_df.join(contract_ids_df, left_on='compilation_id', right_on='compilation_id', how='left')\n",
    "abis_df = abis_df.join(deployment_ids_df, left_on='deployment_id', right_on='deployment_id', how='left')\n",
    "abis_df = abis_df.filter(pl.col('address').is_not_null())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break abis df into 2 files, unique_abis and mapping_contracts_abi_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_abis_df = abis_df.group_by(['abi_id']).agg(pl.first(['abi', 'address']), pl.len().alias('abi_count')).sort(['abi_id'], descending=True)\n",
    "unique_abis_df.write_parquet(\"unique_abis.parquet\")\n",
    "mapping_contracts_abi_id = abis_df.drop(['abi', 'compilation_id', 'deployment_id', 'id']).sort(['chain_id', 'address'])\n",
    "mapping_contracts_abi_id.write_parquet(\"mapping_contracts_abi_id.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each line in the unique_abis file, read the abi and address, and use glaciers to process the abi, generating a dataframe with each item of the abi.\n",
    "\n",
    "Concatenate the dataframes and save them in a new file, in steps of 50000 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abi_df = pl.read_parquet(\"unique_abis.parquet\")\n",
    "abi_df = abi_df.with_columns(pl.concat_str(pl.lit('0x'), pl.col('address').bin.encode(\"hex\")).alias('address'))\n",
    "\n",
    "os.makedirs(\"processed_abis\", exist_ok=True)\n",
    "processed_abi_itens = []\n",
    "i = 0\n",
    "for row in abi_df.iter_rows(named=True):\n",
    "    i += 1\n",
    "    try:\n",
    "        result_df = gl.read_new_abi_json(row['abi'], row['address'])\n",
    "        result_df = result_df.with_columns(pl.lit(row['abi_id']).cast(pl.UInt64).alias('abi_id')).drop(['address'])\n",
    "        processed_abi_itens.append(result_df)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    if i % 50000 == 0:\n",
    "        saving_df = pl.concat(processed_abi_itens)\n",
    "        saving_df.write_parquet(f\"processed_abis/processed_abis_{i}.parquet\")\n",
    "        processed_abi_itens = []\n",
    "saving_df = pl.concat(processed_abi_itens)\n",
    "saving_df.write_parquet(f\"processed_abis/processed_abis_{i}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe with the processed abis and break it into 2 files, unique_abis_itens and mapping_item_abi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_abis = pl.scan_parquet(\"processed_abis/processed_abis_*.parquet\").collect()\n",
    "processed_abis = processed_abis.with_columns(pl.col('full_signature').hash().alias('item_id'))\n",
    "mapping_item_abi = processed_abis.select(['abi_id', 'item_id']).sort(['item_id'])\n",
    "mapping_item_abi.write_parquet(\"mapping_item_abi.parquet\")\n",
    "\n",
    "unique_abis_itens = processed_abis.group_by(['hash', 'full_signature', 'name', 'anonymous', 'num_indexed_args', 'state_mutability', 'item_id']).agg(pl.len().alias('item_count')).sort(['item_count'], descending=True)\n",
    "unique_abis_itens.write_parquet(\"unique_abis_itens.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the events items and join with the mapping_item_abi and mapping_contracts_abi_id to create the ethereum__abis file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ethereum_chain_id = 1\n",
    "\n",
    "unique_abis_itens = pl.read_parquet(\"unique_abis_itens.parquet\")\n",
    "event_abis_itens = unique_abis_itens.filter(pl.col('state_mutability').is_null())\n",
    "mapping_item_abi = pl.read_parquet(\"mapping_item_abi.parquet\")\n",
    "mapping_contracts_abi_id = pl.read_parquet(\"mapping_contracts_abi_id.parquet\")\n",
    "mapping_contracts_abi_id = mapping_contracts_abi_id.filter(pl.col('chain_id') == ethereum_chain_id)\n",
    "\n",
    "ethereum__abis = event_abis_itens.join(mapping_item_abi, left_on='item_id', right_on='item_id', how='left')\n",
    "ethereum__abis = ethereum__abis.join(mapping_contracts_abi_id, left_on='abi_id', right_on='abi_id', how='left').rename({'name_right': 'contract_name', 'fully_qualified_name': 'contract_full_name'})\n",
    "ethereum__abis = ethereum__abis.drop(['abi_id', 'item_id', 'item_count']).filter(pl.col('address').is_not_null())\n",
    "ethereum__abis = ethereum__abis.with_columns(pl.concat_str(\n",
    "        pl.lit('0x'), pl.col('hash').bin.encode(\"hex\").str.to_lowercase(),\n",
    "        pl.lit (' - '), pl.col('full_signature'),\n",
    "        pl.lit (' - '), pl.lit('0x'), pl.col('address').bin.encode(\"hex\").str.to_lowercase()\n",
    "    ).alias('id'))\n",
    "ethereum__abis = ethereum__abis.unique(subset=['id'])\n",
    "\n",
    "ethereum__abis.write_parquet(\"sourcify__ethereum__abis__events.parquet\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
